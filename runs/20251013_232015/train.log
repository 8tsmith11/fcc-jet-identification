Loading classifier from: runs/20251013_231658/classifier.model
[iter 0001] loss=0.58431943
[iter 0002] loss=0.58067398
[iter 0003] loss=0.67222876
[iter 0004] loss=0.65471460
[iter 0005] loss=0.55997007
[iter 0006] loss=0.66179167
[iter 0007] loss=0.61510320
[iter 0008] loss=0.57972135
[iter 0009] loss=0.64905592
[iter 0010] loss=0.62444277
[iter 0011] loss=0.60091489
[iter 0012] loss=0.66277331
[iter 0013] loss=0.61525665
[iter 0014] loss=0.55962394
[iter 0015] loss=0.55939363
[iter 0016] loss=0.56307256
[iter 0017] loss=0.55517760
[iter 0018] loss=0.63658089
[iter 0019] loss=0.65686511
[iter 0020] loss=0.62384388
[iter 0021] loss=0.68338650
[iter 0022] loss=0.64722562
[iter 0023] loss=0.58629107
[iter 0024] loss=0.67498730
[iter 0025] loss=0.71593168
[iter 0026] loss=0.62068254
[iter 0027] loss=0.59426260
[iter 0028] loss=0.67775732
[iter 0029] loss=0.62326770
[iter 0030] loss=0.69791171
[iter 0031] loss=0.70541818
[iter 0032] loss=0.59440557
[iter 0033] loss=0.68328189
[iter 0034] loss=0.75707279
[iter 0035] loss=0.61998942
[iter 0036] loss=0.60522363
[iter 0037] loss=0.56820657
[iter 0038] loss=0.55844158
[iter 0039] loss=0.76564992
[iter 0040] loss=0.71280234
[iter 0041] loss=0.55721395
[iter 0042] loss=0.76585302
[iter 0043] loss=0.71263506
[iter 0044] loss=0.55621394
[iter 0045] loss=0.55785588
[iter 0046] loss=0.55586614
[iter 0047] loss=0.55932835
[iter 0048] loss=0.55754343
[iter 0049] loss=0.55622852
[iter 0050] loss=0.58637624
[iter 0051] loss=0.59750201
[iter 0052] loss=0.61043169
[iter 0053] loss=0.62684303
[iter 0054] loss=0.58450067
[iter 0055] loss=0.64078004
[iter 0056] loss=0.55646014
[iter 0057] loss=0.55609285
[iter 0058] loss=0.55686748
[iter 0059] loss=0.60263772
[iter 0060] loss=0.70695528
[iter 0061] loss=0.78879318
[iter 0062] loss=0.56009725
[iter 0063] loss=0.76641035
[iter 0064] loss=0.71561492
[iter 0065] loss=0.79569892
[iter 0066] loss=0.57915873
[iter 0067] loss=0.79387396
[iter 0068] loss=0.55627284
[iter 0069] loss=0.79778196
[iter 0070] loss=0.57763223
[iter 0071] loss=0.80364992
[iter 0072] loss=0.61247660
[iter 0073] loss=0.55955764
[iter 0074] loss=0.61183041
[iter 0075] loss=0.56621209
[iter 0076] loss=0.61355291
[iter 0077] loss=0.56137195
[iter 0078] loss=0.58398367
[iter 0079] loss=0.55976807
[iter 0080] loss=0.56402257
[iter 0081] loss=0.57149356
[iter 0082] loss=0.56346611
[iter 0083] loss=0.56044571
[iter 0084] loss=0.57730133
[iter 0085] loss=0.55607684
[iter 0086] loss=0.59213414
[iter 0087] loss=0.55771565
[iter 0088] loss=0.57480105
[iter 0089] loss=0.56017832
[iter 0090] loss=0.59891751
[iter 0091] loss=0.56722989
[iter 0092] loss=0.55996808
[iter 0093] loss=0.55274990
[iter 0094] loss=0.56143135
[iter 0095] loss=0.57717736
[iter 0096] loss=0.55188043
[iter 0097] loss=0.57633844
[iter 0098] loss=0.55200385
[iter 0099] loss=0.55844844
[iter 0100] loss=0.56070976
[iter 0101] loss=0.53727592
[iter 0102] loss=0.54274516
[iter 0103] loss=0.54409762
[iter 0104] loss=0.54103747
[iter 0105] loss=0.55260230
[iter 0106] loss=0.54219248
[iter 0107] loss=0.54720959
[iter 0108] loss=0.54199293
[iter 0109] loss=0.54510999
[iter 0110] loss=0.54664331
[iter 0111] loss=0.53706343
[iter 0112] loss=0.53410369
[iter 0113] loss=0.53112230
[iter 0114] loss=0.53141929
[iter 0115] loss=0.53628849
[iter 0116] loss=0.52400403
[iter 0117] loss=0.51721753
[iter 0118] loss=0.51234002
[iter 0119] loss=0.51505672
[iter 0120] loss=0.51219631
[iter 0121] loss=0.50912782
[iter 0122] loss=0.50931413
[iter 0123] loss=0.53267165
[iter 0124] loss=0.50986332
[iter 0125] loss=0.51786924
[iter 0126] loss=0.50741988
[iter 0127] loss=0.49027809
[iter 0128] loss=0.49526789
[iter 0129] loss=0.51304406
[iter 0130] loss=0.49501331
[iter 0131] loss=0.48921749
[iter 0132] loss=0.49556040
[iter 0133] loss=0.49225961
[iter 0134] loss=0.49334560
[iter 0135] loss=0.49925153
[iter 0136] loss=0.49647399
[iter 0137] loss=0.50196419
[iter 0138] loss=0.49648224
[iter 0139] loss=0.49932189
[iter 0140] loss=0.49254667
[iter 0141] loss=0.48219231
[iter 0142] loss=0.48549079
[iter 0143] loss=0.49380285
[iter 0144] loss=0.48818807
[iter 0145] loss=0.50149409
[iter 0146] loss=0.49223051
[iter 0147] loss=0.47250997
[iter 0148] loss=0.48042917
[iter 0149] loss=0.48531719
[iter 0150] loss=0.46926293
[iter 0151] loss=0.48433473
[iter 0152] loss=0.47074775
[iter 0153] loss=0.48218508
[iter 0154] loss=0.47327940
[iter 0155] loss=0.47295684
[iter 0156] loss=0.46960300
[iter 0157] loss=0.48125376
[iter 0158] loss=0.48177015
[iter 0159] loss=0.47431130
[iter 0160] loss=0.46098956
[iter 0161] loss=0.45585732
[iter 0162] loss=0.45130544
[iter 0163] loss=0.46106755
[iter 0164] loss=0.45376150
[iter 0165] loss=0.45937400
[iter 0166] loss=0.45177140
[iter 0167] loss=0.45034387
[iter 0168] loss=0.45428651
[iter 0169] loss=0.44704111
[iter 0170] loss=0.45479508
[iter 0171] loss=0.44460070
[iter 0172] loss=0.45016999
[iter 0173] loss=0.44931540
[iter 0174] loss=0.44855652
[iter 0175] loss=0.44804757
[iter 0176] loss=0.44581485
[iter 0177] loss=0.44908835
[iter 0178] loss=0.44791427
[iter 0179] loss=0.44828713
[iter 0180] loss=0.44469716
[iter 0181] loss=0.45062996
[iter 0182] loss=0.45166806
[iter 0183] loss=0.47300137
[iter 0184] loss=0.44846594
[iter 0185] loss=0.44448288
[iter 0186] loss=0.45018911
[iter 0187] loss=0.45566592
[iter 0188] loss=0.44897881
[iter 0189] loss=0.45967321
[iter 0190] loss=0.44809595
[iter 0191] loss=0.44462660
[iter 0192] loss=0.44979232
[iter 0193] loss=0.47424141
[iter 0194] loss=0.44643948
[iter 0195] loss=0.44912547
[iter 0196] loss=0.44805835
[iter 0197] loss=0.44078401
[iter 0198] loss=0.44162007
[iter 0199] loss=0.43635500
[iter 0200] loss=0.43604166
Accuracy from the train data : 97.86%
Accuracy from the test data : 98.33%
Saved classifier to runs/20251013_231658/classifier.model
Saved objective curve to runs/20251013_232015/objective_curve.png
